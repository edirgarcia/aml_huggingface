{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace\n",
    "\n",
    "import pandas as pd\n",
    "from tokenizers import decoders, models, pre_tokenizers, processors, trainers, Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get the workspace and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1635877467526
    }
   },
   "outputs": [],
   "source": [
    "# This can be run both locally using the explicit\n",
    "# workspace = Workspace.from_config(\"../aml_workspace_config.json\")\n",
    "# or from an Azure Compute using only:\n",
    "# workspace = Workspace.from_config()\n",
    "# This below only works when from an Azure compute, if you're in another env you should name it explicitly.\n",
    "workspace = Workspace.from_config()\n",
    "default_datastore = workspace.get_default_datastore()\n",
    "default_datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get the dataset from the HuggingFace datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1635877477874
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# # This is commented out, because this is simply not realistic only for didatic purposes.\n",
    "# # On your scenario this will very likely come from a Blob location and it was put there by Azure Data Factory or other copying service.\n",
    "# # However for the sake of this workshop, if you are not on the cybersai-innovation workspace, then you can use this data set instead.\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistically get the data from a Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this resembles more the actual flow, something already put the data on Azure Blob you know of, usually the one associated with the Azure Machine Learning workspace\n",
    "from azureml.core import Dataset\n",
    "datastore_paths = [(default_datastore, 'imdb/data/imdb_unsupervised.csv')]\n",
    "dataset = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "#convert dataset to a dataframe\n",
    "df = dataset.to_pandas_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you pick a model for your tokenizer\n",
    "# https://huggingface.co/docs/tokenizers/python/latest/components.html#models\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "#you pick a pre-tokenizer for your tokenizer\n",
    "#https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "#test the pre-tokenizer\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#special tokens for RoBERTa model\n",
    "special_tokens = [\n",
    "                \"<s>\",\n",
    "                \"<pad>\",\n",
    "                \"</s>\",\n",
    "                \"<mask>\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30522 size comes from original RoBERTa config\n",
    "# https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers\n",
    "trainer = trainers.BpeTrainer(vocab_size=30522, special_tokens=special_tokens)\n",
    "\n",
    "# actually train the tokenizer\n",
    "tokenizer.train_from_iterator(df['text'], trainer=trainer)\n",
    "\n",
    "#did you use the HuggingFace dataset? use this below instead\n",
    "#tokenizer.train_from_iterator(dataset[\"unsupervised\"][\"text\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer, or tokenization\")\n",
    "print(encoding.ids)\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply post processor\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "\n",
    "# set the decoder\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "#this to see post processor working\n",
    "sentence = df['text'][0]\n",
    "#did you use the HuggingFace dataset? use this instead\n",
    "# sentence = dataset[\"unsupervised\"][\"text\"][0]\n",
    "\n",
    "print(\"sentence: \", sentence)\n",
    "encoding = tokenizer.encode(sentence)\n",
    "print(\"encoding ids\", encoding.ids)\n",
    "print(\"encoding offsets\", encoding.offsets)\n",
    "start, end = encoding.offsets[4]\n",
    "print(\"start: \", start)\n",
    "print(\"end: \", end)\n",
    "sentence[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test it all works\n",
    "print(encoding.ids)\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#wrap it in a fast tokenizer to export to use in transformers library\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "\n",
    "wrapped_tokenizer.save_pretrained(\"../src/tokenizers/imdb_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like looking at the vocabulary, you can find all kinds of interesting things in there\n",
    "wrapped_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if you want to train a different type of Tokenizer? like say a Word Level Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you pick a model for your tokenizer\n",
    "word_tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "#you pick a pre-tokenizer for your tokenizer\n",
    "word_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer(add_prefix_space=False)\n",
    "#test the pre-tokenizer\n",
    "word_tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")\n",
    "\n",
    "\n",
    "word_special_tokens = [\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\",\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "]\n",
    "\n",
    "# 30522 size comes from original BERT config\n",
    "word_trainer = trainers.WordLevelTrainer(vocab_size=30522, special_tokens=word_special_tokens)\n",
    "\n",
    "# actually train the tokenizer\n",
    "word_tokenizer.train_from_iterator(df['text'], trainer=word_trainer)\n",
    "\n",
    "#did you use the HuggingFace dataset? use this below instead\n",
    "#tokenizer.train_from_iterator(dataset[\"unsupervised\"][\"text\"], trainer=trainer)\n",
    "\n",
    "word_encoding = word_tokenizer.encode(\"Let's test this tokenizer, or tokenization\")\n",
    "print(word_encoding.ids)\n",
    "print(word_encoding.tokens)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91e1ace8ed40482a220e2888987fa996506e868337e751bc59241b473aada2c4"
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
