{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Dataset, Experiment, ContainerRegistry\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"Using AzureML SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Use existing workspace\n",
    "\"\"\"\n",
    "ws = Workspace.from_config(\"../aml_workspace_config.json\")\n",
    "\n",
    "print(\"Using workspace: {} @ location: {}\".format(ws.name, ws.location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##  Get The default Datastore\n",
    "\"\"\"\n",
    "default_datastore = ws.get_default_datastore()\n",
    "print(\"Default Datastore: %s\" % default_datastore.account_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##  Use existing compute cluster\n",
    "\"\"\"\n",
    "compute_name = \"GPU-NC24S\" \n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"Found compute target. Using '{}' compute \".format( compute_name))\n",
    "    else:\n",
    "        print('Compute Target Not found. Create Manually.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##  Get Secret stuff\n",
    "\"\"\"\n",
    "keyvault = ws.get_default_keyvault()\n",
    "\n",
    "registry_user = keyvault.get_secret(\"registry-user\")\n",
    "registry_password = keyvault.get_secret(\"registry-password\")\n",
    "\n",
    "registry_address = \"5027c9a8fca54c36927f93253a076626.azurecr.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#curated_env_name = 'AzureML-pytorch-1.7-ubuntu18.04-py37-cuda11-gpu'\n",
    "curated_env_name = 'AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu'\n",
    "pytorch_env = Environment.get(workspace=ws, name=curated_env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# `compute_name` as defined in \"Azure Machine Learning compute\" section above\n",
    "aml_run_config.target = compute_name\n",
    "\n",
    "registry = ContainerRegistry()\n",
    "registry.address = registry_address\n",
    "#this below was needed in cybersai workspace, must investigate why\n",
    "registry.username = registry_user\n",
    "registry.password = registry_password\n",
    "aml_run_config.environment.docker.base_image_registry = registry\n",
    "aml_run_config.framework = 'Python'\n",
    "aml_run_config.environment.docker.enabled = True\n",
    "#aml_run_config.environment.docker.base_image = \"openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04\" \n",
    "aml_run_config.environment.docker.base_image = \"pytorch1.9-openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04\"\n",
    "#does this work? \n",
    "aml_run_config.environment.docker.shm_size  = \"16g\"\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "aml_run_config.environment.python.interpreter_path = 'python3'\n",
    "\n",
    "# Add some packages relied on by data prep step\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    #conda_packages=['pytorch'], \n",
    "    #PyTorch Lightnining now needs to be exactly this, some breaking changes have been introduced. Distributed scoring?\n",
    "                                                                                                                  #these come from reqiurements.txt from the run_mlm script\n",
    "    pip_packages=['azureml-sdk', 'numpy', 'pandas', 'transformers','sklearn' , 'datasets','accelerate', 'sentencepiece', 'protobuf'], \n",
    "    pin_sdk_version=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be explicitly named Datasets\n",
    "#roberta_ps_train = Dataset.get_by_name(ws, name='roberta_ps_train')\n",
    "#roberta_ps_test = Dataset.get_by_name(ws, name='roberta_ps_test')\n",
    "\n",
    "# or not explicitly named Datasets, I like these better they're less work to maintain\n",
    "datastore_paths = [(default_datastore, 'imdb/data/imdb_unsupervised.csv')]\n",
    "#dataset = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
    "dataset = Dataset.File.from_files(path=datastore_paths)\n",
    "dataset\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create or Refer to an script folder on your local machine\n",
    "script_folder = os.path.join(os.getcwd(), \"..\\src\" )\n",
    "print(\"script_folder\",script_folder)\n",
    "\n",
    "train_output = PipelineData(\"train_output\", datastore=default_datastore)\n",
    "\n",
    "train_consumption_conf = dataset.as_download()\n",
    "test_consumption_conf = dataset.as_download()\n",
    "\n",
    "# --model_type roberta will train from scratch\n",
    "# --model_name_or_path roberta-base will use the roberta-base pre-train\n",
    "\n",
    "#training from scratch\n",
    "from_sratch_arguments = [ '--model_type', 'roberta',  '--train_file', train_consumption_conf, \n",
    "               '--validation_file', test_consumption_conf, '--do_train', '--do_eval', '--num_train_epochs', 5, '--save_steps', 10000, # comment this out later\n",
    "               '--output_dir', train_output, '--tokenizer_name', 'tokenizers/imdb_tokenizer',\n",
    "              '--max_seq_length', 512, '--per_device_train_batch_size', 4, '--fp16', True,\n",
    "              '--config_overrides', 'max_position_embeddings=514']\n",
    "\n",
    "# fine tuning\n",
    "# https://huggingface.co/roberta-base \n",
    "fine_tune_arguments = [ '--model_name_or_path', 'roberta-base',  '--train_file', train_consumption_conf, \n",
    "               '--validation_file', test_consumption_conf, '--do_train', '--do_eval', '--num_train_epochs', 5, '--save_steps', 10000, # comment this out later\n",
    "               '--output_dir', train_output, '--max_seq_length', 512, '--per_device_train_batch_size', 4, '--fp16', True]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mlm_step = PythonScriptStep(\n",
    "    script_name=\"run_mlm.py\",\n",
    "    source_directory=script_folder,\n",
    "    inputs=[train_consumption_conf, test_consumption_conf],\n",
    "    outputs=[train_output],\n",
    "    arguments=fine_tune_arguments,\n",
    "    compute_target=compute_target,\n",
    "    runconfig=aml_run_config,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "imdb_roberta_pipeline = Pipeline(workspace=ws, steps=[train_mlm_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe rename to imdb_roberta_pipeline\n",
    "experiment = Experiment(ws, 'imdb_roberta_pipeline')\n",
    "pipeline_run = experiment.submit(imdb_roberta_pipeline)\n",
    "\n",
    "#pipeline_run.set_tags({'ModelType':'RoBERTa from Scratch'})\n",
    "pipeline_run.set_tags({'ModelType':'RoBERTa Base Fine-tuning'})\n",
    "#pipeline_run.wait_for_completion()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91e1ace8ed40482a220e2888987fa996506e868337e751bc59241b473aada2c4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('ps_bert': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
